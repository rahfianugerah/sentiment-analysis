# -*- coding: utf-8 -*-
"""nlp_twitter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_h4KQq_WqMG-kPTltjBf8_EIOezD7t7p

# Natural Language Processing: Twitter Sentiment Analysis using TensorFlow with LSTM

## Install Kaggle and Config Kaggle
"""

# Install Kaggle using pip
!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

"""## Download Dataset from Kaggle"""

# Download dataset
!kaggle datasets download -d jp797498e/twitter-entity-sentiment-analysis

"""## Import Required Library"""

# Import requried library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb
import tensorflow as tf
import zipfile
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical

"""## Extract File Zip"""

# Extract file
data_dir = "/content/"
zip_dir = f"{data_dir}/twitter-entity-sentiment-analysis.zip"
zip = zipfile.ZipFile(zip_dir, 'r')
zip.extractall('/content')
zip.close()

"""## Open File Dataset"""

# Open trainin dataset
train = pd.read_csv(f"{data_dir}twitter_training.csv")
train.head()

val = pd.read_csv(f"{data_dir}twitter_validation.csv")
val.head()

"""## Change Columns Name"""

# Change column names for train and test dataset
val.columns = ['Header', 'Env','Label','Text']
train.columns = ['Header', 'Env','Label','Text']

train.head()

val.head()

"""## Concate Train set and Validation set"""

data = pd.concat([train, val], ignore_index=True)
data.head(len(data))

print(f"Data Shape: {data.shape}")

"""## Data Cleaning"""

data.info()

print(f"Null Data:\n{data.isna().sum()}")

print(f"Duplicated Data: {data.duplicated().sum()}")

data.dropna(inplace=True)
print(f"Null Data:\n{data.isna().sum()}")

data.drop_duplicates(inplace=True)
print(f"Duplicated Data: {data.duplicated().sum()}")

data = data.drop(columns=['Header'])
data.head()

"""## Create Label Count Plot"""

# Countplot Emotion
sb.countplot(data.Label)

"""## Import NTLK and Other Required Library"""

# Import Required Library
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize,sent_tokenize
from bs4 import BeautifulSoup
import re,string,unicodedata
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.model_selection import train_test_split
from string import punctuation
from nltk import pos_tag
from nltk.corpus import wordnet
import keras
from keras.preprocessing import text, sequence
import nltk
nltk.download('stopwords')

"""## Stopwords"""

stwd = set(stopwords.words('english'))
punctuation = list(string.punctuation)
stwd.update(punctuation)

"""## Cleaning Text"""

import re
from bs4 import BeautifulSoup
from nltk.corpus import stopwords

stwd = stopwords.words('english')

def clean_text(text):

    cleaned_text = text
    cleaned_text = strip_html(cleaned_text)
    cleaned_text = remove_between_square_brackets(cleaned_text)
    cleaned_text = remove_url(cleaned_text)
    cleaned_text = remove_stopwords(cleaned_text)

    return cleaned_text

def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

def remove_url(text):
    return re.sub(r'http\S+', '', text)

def remove_stopwords(text):
    final_text = []
    for word in text.split():
        if word.strip().lower() not in stwd:
            final_text.append(word.strip())
    return " ".join(final_text)

data['Text'] = data['Text'].apply(clean_text)

def get_corpus(text):
    words = []
    for i in text:
        for j in i.split():
            words.append(j.strip())
    return words
corpus = get_corpus(data.Text)
corpus[:10]

# Total Common Words
from collections import Counter
counter = Counter(corpus)
most_common = counter.most_common(10)
most_common = dict(most_common)
most_common

from sklearn.feature_extraction.text import CountVectorizer
def get_top_text_ngrams(corpus, n, g):
    cv = CountVectorizer(ngram_range=(g, g)).fit(corpus)
    bag_words = cv.transform(corpus)
    sum_words = bag_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

"""## Most Common Words Plot"""

plt.figure(figsize = (16,9))
most_common = get_top_text_ngrams(data.Text, 10, 1)
most_common = dict(most_common)
sb.barplot(x=list(most_common.values()),y=list(most_common.keys()))

"""## One - Hot Encoding"""

# Data Label One-Hot Encoding
Label = pd.get_dummies(data.Label, dtype=int)
new_data = pd.concat([data, Label], axis=1)
new_data = new_data.drop(columns='Label')
new_data.head(10)

"""## Splitting Dataset"""

import string
from collections import Counter

def process_text(text):
  """
  Preprocesses text data for further analysis.

  Args:
      text (str): The text to be processed.

  Returns:
      list: A list of cleaned and frequent words.
  """

  # Step 1: Remove punctuation (except apostrophes) and convert to lowercase
  text = ''.join(c for c in text.lower() if c in string.ascii_letters + "'")

  # Step 2: Tokenize the text
  words = text.split()

  # Step 3: Remove infrequent words (optional, customize threshold)
  word_counts = Counter(words)
  min_count = 2  # Adjust this threshold as needed
  frequent_words = [word for word, count in word_counts.items() if count >= min_count]

  return frequent_words

data['Text'] = data['Text'].apply(process_text)

datas = new_data['Text'].values
category = new_data[['Irrelevant',	'Negative',	'Neutral',	'Positive']].values

x_train,x_test,y_train,y_test = train_test_split(datas, category, test_size = 0.2, shuffle=True)

"""## Create Callbacks"""

# Create Callback
class Callback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.9 and logs.get('val_accuracy') > 0.9):
      print(f"Accuracy have been achieved!\nAcc Train: {logs.get('accuracy')}\nAcc Val: {logs.get('val_accuracy')}")
      self.model.stop_training = True
callbacks= Callback()

"""## Tokenizer"""

vocab_size = 10000
max_len = 100
trunc_type = "post"
oov_tok = "<oov>"

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(x_train)

word_index = tokenizer.word_index

sequences_train = tokenizer.texts_to_sequences(x_train)
sequences_test = tokenizer.texts_to_sequences(x_test)
pad_train = pad_sequences(sequences_train, maxlen=max_len, truncating=trunc_type)
pad_test = pad_sequences(sequences_test, maxlen=max_len, truncating=trunc_type)

"""## Create Model"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(4, activation='softmax')
])
model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy',)
model.summary()

"""## Train Model"""

history = model.fit(pad_train, y_train, epochs=30, validation_data=(pad_test, y_test), batch_size=64, verbose=2)

"""## Model Evaluation"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

# Create a subplot figure
plt.figure(figsize=(20, 5))  # Adjust figure size as needed

# Subplot for Accuracy
plt.subplot(1, 2, 1)  # 1 row, 2 columns, subplot 1
plt.plot(epochs, acc, 'r', label='Train Accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')
plt.title('Model Accuracy')
plt.legend(loc=0)

# Subplot for all
plt.subplot(1, 2, 2)  # 1 row, 2 columns, subplot 2
plt.plot(epochs, acc, 'blue', label='Train Accuracy')
plt.plot(epochs, val_acc, 'green', label='Validation Accuracy')
plt.plot(epochs, loss, 'red', label='Train Loss')
plt.plot(epochs, val_loss, 'orange', label='Validation Loss')
plt.title('Model Accuracy and Loss')
plt.legend(loc=0)


# Adjust layout to prevent overlapping elements
plt.tight_layout()

plt.show()
